{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhnb9kmiJoRp2lESvx0ez8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roysouvik2/PINN_optimal/blob/main/optimal1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the PINN model\n",
        "class PINN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(PINN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(40, activation='tanh')  # Increased neurons\n",
        "        self.dense2 = tf.keras.layers.Dense(40, activation='tanh')\n",
        "        self.dense3 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x, t):\n",
        "        inputs = tf.concat([x, t], axis=1)\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "# Define the PDE loss for the heat equation\n",
        "def pde_loss(model, x, t, alpha):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch([x, t])\n",
        "        u = model(x, t)\n",
        "        u_x = tape.gradient(u, x)\n",
        "        u_xx = tape.gradient(u_x, x)\n",
        "        u_t = tape.gradient(u, t)\n",
        "        residual = u_t - alpha * u_xx\n",
        "\n",
        "    loss_de = tf.reduce_mean(tf.square(residual))\n",
        "    return loss_de\n",
        "\n",
        "# Define the cost function for the optimal control problem\n",
        "def cost_function(model, x, t_target, u_target):\n",
        "    t_target_broadcast = tf.broadcast_to(t_target, tf.shape(x))\n",
        "    u_pred = model(x, t_target_broadcast)\n",
        "    loss_cost = tf.reduce_mean(tf.square(u_pred - u_target))\n",
        "    return loss_cost\n",
        "\n",
        "# Define the initial condition as a trainable variable\n",
        "def initial_condition(x):\n",
        "    init_cond = tf.Variable(tf.ones(x.shape), dtype=tf.float32, trainable=True)\n",
        "    return init_cond\n",
        "\n",
        "# Define the training step with gradient clipping and loss balancing\n",
        "def train_step(model, x, t, x0, u0, alpha, optimizer, clip_norm, weight_pde, weight_func):\n",
        "    with tf.GradientTape() as tape:\n",
        "        t0 = tf.zeros_like(x0)  # time t = 0\n",
        "        u_pred0 = model(x0, t0)\n",
        "\n",
        "        #ic_loss_value = tf.reduce_mean(tf.square(u_pred0 - tf.convert_to_tensor(u0)))\n",
        "        pde_loss_value = pde_loss(model, x, t, alpha)\n",
        "        func_loss_value = cost_function(model, x, t_target, u_target)\n",
        "\n",
        "        total_loss = (weight_pde * pde_loss_value +\n",
        "                      weight_func * func_loss_value)\n",
        "\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    grads, _ = tf.clip_by_global_norm(grads, clip_norm)  # Gradient clipping\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return pde_loss_value, func_loss_value, total_loss\n",
        "\n",
        "# Training function with learning rate scheduler and loss balancing\n",
        "def train_optimal_control(model, x, t, x0, u0, alpha, u_target, t_target, epochs=3000):\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=0.001,\n",
        "        decay_steps=1000,\n",
        "        decay_rate=0.9\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_pde_loss = 0\n",
        "        total_ic_loss = 0\n",
        "        total_func_loss = 0\n",
        "        total_loss = 0\n",
        "\n",
        "        pde_loss_value, func_loss_value, current_loss = train_step(\n",
        "            model, x, t, x0, u0, alpha, optimizer,\n",
        "            clip_norm = 1.0,\n",
        "            weight_pde=1.0,  # Weight for PDE loss\n",
        "            weight_func=10.0,  # Increased weight for functional loss\n",
        "        )\n",
        "\n",
        "        total_pde_loss += pde_loss_value.numpy()\n",
        "        #total_ic_loss += ic_loss_value.numpy()\n",
        "        total_func_loss += func_loss_value.numpy()\n",
        "        total_loss += current_loss.numpy()\n",
        "\n",
        "        if epoch % 1000 == 0:\n",
        "            print(f'Epoch {epoch}: PDE Loss = {total_pde_loss:.3e}, '\n",
        "                  f'Functional Loss = {total_func_loss:.3e}, '\n",
        "                  f'Total Loss = {total_loss:.3e}')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Generate training data\n",
        "x_train = tf.random.uniform((100, 1), 0.0, 1.0)\n",
        "t_train = tf.random.uniform((100, 1), 0.0, 1.0)\n",
        "\n",
        "# Define the model\n",
        "model = PINN()\n",
        "\n",
        "# Define the thermal diffusivity constant\n",
        "alpha = 0.01\n",
        "\n",
        "# Define the target time and the desired state at that time\n",
        "t_target = tf.constant([[1.0]], dtype=tf.float32)\n",
        "u_target = tf.constant(np.sin(np.pi * np.linspace(0, 1, 100)).reshape(-1, 1) * np.exp(-alpha * np.pi**2), dtype=tf.float32)\n",
        "\n",
        "# Define the initial condition (as a trainable variable)\n",
        "x0 = tf.linspace(0.0, 1.0, 100)[:, tf.newaxis]\n",
        "u0 = initial_condition(x0)\n",
        "\n",
        "# Train the model to find the optimal initial condition\n",
        "train_optimal_control(model, x_train, t_train, x0, u0, alpha, u_target, t_target)\n",
        "\n",
        "# Validate the solution\n",
        "def validate_optimal_solution(model, x0, u0, u_target, alpha):\n",
        "    x_test = tf.linspace(0.0, 1.0, 100)[:, tf.newaxis]\n",
        "    t_test = tf.constant([[1.0]], dtype=tf.float32)\n",
        "    t_test_broadcast = tf.broadcast_to(t_test, tf.shape(x_test))\n",
        "    u_pred = model(x_test, t_test_broadcast)\n",
        "    u_pred = tf.reshape(u_pred, (100, 1))\n",
        "\n",
        "    plt.plot(x0, u_pred, label='Prediction at t=1.0')\n",
        "    plt.plot(x0, u_target, label='Target State at t=1.0', linestyle='--')\n",
        "    plt.plot(x0, u0.numpy(), label='Optimized Initial Condition', linestyle='-.')  # Convert u0 to numpy for plotting\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Validate the result\n",
        "validate_optimal_solution(model, x0, u0, u_target, alpha)\n"
      ],
      "metadata": {
        "id": "0soayvdc6kHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}